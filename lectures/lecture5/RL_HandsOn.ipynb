{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Hands-on example: Pong\n",
    "\n",
    "### References:\n",
    "- http://karpathy.github.io/2016/05/31/rl/\n",
    "- https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup:\n",
    "\n",
    "For instructions please refer to this blog post:\n",
    "https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0\n",
    "\n",
    "Installing required packages can be tricky and error prone. Thus, we recommend reading the post carefully.\n",
    "\n",
    "Setup:\n",
    "\n",
    "1. Open up the code to follow along.\n",
    "2. Follow the instructions for installing OpenAI Gym. You may need to install cmake first.\n",
    "3. Run pip install -e .[atari]\n",
    "4. Go back to the root of this repository and open a new file called my_pong.py in your favorite editor.\n",
    "5. Let’s get to problem solving. Here’s the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "We are given the following:\n",
    "\n",
    "- A sequence of images (frames) representing each frame of the Pong game.\n",
    "- An indication when we’ve won or lost the game.\n",
    "- An opponent agent that is the traditional Pong computer player.\n",
    "- An agent we control that we can tell to move up or down at each frame.\n",
    "\n",
    "Can we use these pieces to train our agent to beat the computer? Moreover, can we make our solution generic enough so it can be reused to win in games that aren’t pong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "\n",
    "# Take in inputs from the screen and preprocess them\n",
    "# Pass them into an NN\n",
    "# Update the weights of the NN using gradient descent\n",
    "# weights['1'] - Matrix that holds weights of pixels passing into hidden layer. Dimensions: [200 x (80 x 80)] -> [200 x 6400]\n",
    "# weights['2'] - Matrix that holds weights of hidden layer passing into output. Dimensions: [1 x 200]\n",
    "\n",
    "# Process is:\n",
    "\n",
    "# processed_observations = image vector - [6400 x 1] array\n",
    "# Compute hidden_layer_values = weights['1'] dot processed_observations ([200 x 6400] dot [6400 x 1]) -> [200 x 1] - \n",
    "    #this gives initial activation values.\n",
    "# Next we need to transform those either via a sigmoid or an ReLU of some sort. Let's use ReLU\n",
    "# ReLU(hidden_layer_values)\n",
    "# Next we need to pass this one layer further\n",
    "# output_layer_value = weights['2'] dot hidden_layer_values ([1 x 200] dot [200 x 1] -> [1 x 1])\n",
    "# Now our output layer is the probability of going up or down. Let's make sure this output is between 0 and 1 \n",
    "    #by passing it through a sigmoid\n",
    "# p = sigmoid(output_layer_value)\n",
    "\n",
    "# Learning after round has finished:\n",
    "\n",
    "# Figure out the result\n",
    "# Compute the error(reward)\n",
    "# Use the error to calculate the gradient\n",
    "    # The below dimensions all assume we had exactly 10 frames in the round (not necessarily true!)\n",
    "    # dC_dw2 = hidden_layer_values^T dot gradient_log_p ([1 x 2000] dot [2000 x 1] -> 1x1)\n",
    "    # delta_1 = gradient_log_p outer_product weights['2'] = [2000 x 1] outer_product [1 x 200] ([2000 x 200])\n",
    "    # dC_dw1 = delta_1^T dot input_observations ([200 x 2000]x dot [2000 x 64000] -> [200 x 64000])\n",
    "\n",
    "# After some batch size of rounds has finished,\n",
    "    # Use rmsprop to move weights['1'] and weights['2'] in the direction of the gradient\n",
    "# Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PG Network](https://miro.medium.com/max/966/1*05ExQKJ0nOoWV80SNVEyJg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def downsample(image):\n",
    "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_color(image):\n",
    "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_background(image):\n",
    "    image[image == 144] = 0\n",
    "    image[image == 109] = 0\n",
    "    return image\n",
    "\n",
    "def preprocess_observations(input_observation, prev_processed_observation, input_dimensions):\n",
    "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = downsample(processed_observation)\n",
    "    processed_observation = remove_color(processed_observation)\n",
    "    processed_observation = remove_background(processed_observation)\n",
    "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
    "    processed_observation = processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    # subtract the previous frame from the current one so we are only processing on changes in the game\n",
    "    if prev_processed_observation is not None:\n",
    "        input_observation = processed_observation - prev_processed_observation\n",
    "    else:\n",
    "        input_observation = np.zeros(input_dimensions)\n",
    "    # store the previous frame so we can subtract from it next time\n",
    "    prev_processed_observations = processed_observation\n",
    "    return input_observation, prev_processed_observations\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def relu(vector):\n",
    "    vector[vector < 0] = 0\n",
    "    return vector\n",
    "\n",
    "def apply_neural_nets(observation_matrix, weights):\n",
    "    \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
    "    hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
    "    hidden_layer_values = relu(hidden_layer_values)\n",
    "    output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
    "    output_layer_values = sigmoid(output_layer_values)\n",
    "    return hidden_layer_values, output_layer_values\n",
    "\n",
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        # signifies up in openai gym\n",
    "        return 2\n",
    "    else:\n",
    "         # signifies down in openai gym\n",
    "        return 3\n",
    "\n",
    "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
    "    \"\"\" See here: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
    "    delta_L = gradient_log_p\n",
    "    dC_dw2 = np.dot(hidden_layer_values.T, delta_L).ravel()\n",
    "    delta_l2 = np.outer(delta_L, weights['2'])\n",
    "    delta_l2 = relu(delta_l2)\n",
    "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
    "    return {\n",
    "        '1': dC_dw1,\n",
    "        '2': dC_dw2\n",
    "    }\n",
    "\n",
    "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
    "    \"\"\" See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
    "    epsilon = 1e-5\n",
    "    for layer_name in weights.keys():\n",
    "        g = g_dict[layer_name]\n",
    "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
    "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" Actions you took 20 steps before the end result are less important to the overall result than an action you took \n",
    "    a step ago.\n",
    "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        if rewards[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return gradient_log_p * discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ## Initialization\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    observation = env.reset() # This gets us the image\n",
    "\n",
    "    # hyperparameters\n",
    "    episode_number = 0\n",
    "    batch_size = 10\n",
    "    gamma = 0.99 # discount factor for reward\n",
    "    decay_rate = 0.99\n",
    "    num_hidden_layer_neurons = 200\n",
    "    input_dimensions = 80 * 80\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    episode_number = 0\n",
    "    reward_sum = 0\n",
    "    running_reward = None\n",
    "    prev_processed_observations = None\n",
    "\n",
    "    weights = {\n",
    "        '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions),\n",
    "        '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
    "    }\n",
    "\n",
    "    # To be used with rmsprop algorithm (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "    expectation_g_squared = {}\n",
    "    g_dict = {}\n",
    "    for layer_name in weights.keys():\n",
    "        expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
    "\n",
    "    episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "        \n",
    "        hidden_layer_values, up_probability = apply_neural_nets(processed_observations, weights)\n",
    "        episode_observations.append(processed_observations)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "\n",
    "        action = choose_action(up_probability)\n",
    "\n",
    "        # carry out the chosen action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_sum += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
    "        fake_label = 1 if action == 2 else 0\n",
    "        loss_function_gradient = fake_label - up_probability\n",
    "        episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # Combine the following values for the episode\n",
    "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "            episode_observations = np.vstack(episode_observations)\n",
    "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "            episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "            # Tweak the gradient of the log_ps based on the discounted rewards\n",
    "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "            gradient = compute_gradient(\n",
    "              episode_gradient_log_ps_discounted,\n",
    "              episode_hidden_layer_values,\n",
    "              episode_observations,\n",
    "              weights\n",
    "            )\n",
    "\n",
    "            # Sum the gradient for use when we hit the batch size\n",
    "            for layer_name in gradient:\n",
    "                g_dict[layer_name] += gradient[layer_name]\n",
    "\n",
    "            if episode_number % batch_size == 0:\n",
    "                update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "\n",
    "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] # reset values\n",
    "            observation = env.reset() # reset env\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "            reward_sum = 0\n",
    "            prev_processed_observations = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -21.000000. running mean: -21.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.980000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980200\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980398\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980594\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980788\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980980\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.971170\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.971459\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.971744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-81626804dfee>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# carry out the chosen action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ismail_env\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ismail_env\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ismail_env\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
